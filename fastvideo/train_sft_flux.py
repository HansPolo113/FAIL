# Copyright (c) [2025] [FastVideo Team]
# Copyright (c) [2025] [ByteDance Ltd. and/or its affiliates.]
# SPDX-License-Identifier: [Apache License 2.0]
#
# This file has been modified by [ByteDance Ltd. and/or its affiliates.] in 2025.
#
# Original file was released under [Apache License 2.0], with the full license text
# available at [https://github.com/hao-ai-lab/FastVideo/blob/main/LICENSE].
#
# This modified file is released under the same license.

"""
Supervised Fine-Tuning (SFT) / Behavior Cloning for FLUX model.

This script implements standard supervised fine-tuning using teacher images
generated by a reference model. It uses the original FLUX flow matching loss
to train the model to directly match the teacher outputs.
"""

import argparse
import os
from fastvideo.utils.parallel_states import (
    initialize_sequence_parallel_state,
    destroy_sequence_parallel_group,
    get_sequence_parallel_state,
)
# Removed: from fastvideo.utils.communications_flux import sp_parallel_dataloader_wrapper
import time
from torch.utils.data import DataLoader
import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.checkpoint.state_dict import get_model_state_dict, set_model_state_dict, StateDictOptions
from torch.utils.data.distributed import DistributedSampler
import wandb
from accelerate.utils import set_seed
from tqdm.auto import tqdm
from fastvideo.utils.fsdp_util import get_dit_fsdp_kwargs, apply_fsdp_checkpointing
from diffusers.optimization import get_scheduler
from diffusers.utils import check_min_version
from fastvideo.dataset.latent_flux_sft_datasets import FluxSFTLatentDataset, flux_sft_latent_collate_function
import torch.distributed as dist
from fastvideo.utils.checkpoint import save_checkpoint
from fastvideo.utils.logging_ import main_print
from diffusers import FluxTransformer2DModel, FluxPipeline
from contextlib import contextmanager
from safetensors.torch import save_file
from collections import deque
import torch.nn.functional as F

check_min_version("0.31.0")


class FSDP_EMA:
    """Exponential Moving Average handler for FSDP models."""
    def __init__(self, model, decay, rank):
        self.decay = decay
        self.rank = rank
        self.ema_state_dict_rank0 = {}
        options = StateDictOptions(full_state_dict=True, cpu_offload=True)
        state_dict = get_model_state_dict(model, options=options)

        if self.rank == 0:
            self.ema_state_dict_rank0 = {k: v.clone() for k, v in state_dict.items()}
            main_print("--> EMA handler initialized on rank 0.")

    def update(self, model):
        options = StateDictOptions(full_state_dict=True, cpu_offload=True)
        model_state_dict = get_model_state_dict(model, options=options)

        if self.rank == 0:
            for key in self.ema_state_dict_rank0:
                if key in model_state_dict:
                    self.ema_state_dict_rank0[key].copy_(
                        self.decay * self.ema_state_dict_rank0[key] + (1 - self.decay) * model_state_dict[key]
                    )

    @contextmanager
    def use_ema_weights(self, model):
        backup_options = StateDictOptions(full_state_dict=True, cpu_offload=True)
        backup_state_dict_rank0 = get_model_state_dict(model, options=backup_options)

        load_options = StateDictOptions(full_state_dict=True, broadcast_from_rank0=True)
        set_model_state_dict(
            model,
            model_state_dict=self.ema_state_dict_rank0,
            options=load_options
        )

        try:
            yield
        finally:
            restore_options = StateDictOptions(full_state_dict=True, broadcast_from_rank0=True)
            set_model_state_dict(
                model,
                model_state_dict=backup_state_dict_rank0,
                options=restore_options
            )


def save_ema_checkpoint(ema_handler, rank, output_dir, step, epoch, config_dict):
    """Save EMA model checkpoint."""
    if rank == 0 and ema_handler is not None:
        ema_checkpoint_path = os.path.join(output_dir, f"checkpoint-ema-{step}-{epoch}")
        os.makedirs(ema_checkpoint_path, exist_ok=True)
        weight_path = os.path.join(ema_checkpoint_path, "diffusion_pytorch_model.safetensors")
        save_file(ema_handler.ema_state_dict_rank0, weight_path)
        if "dtype" in config_dict:
            del config_dict["dtype"]
        config_path = os.path.join(ema_checkpoint_path, "config.json")
        import json
        with open(config_path, "w") as f:
            json.dump(config_dict, f, indent=4)
        main_print(f"--> EMA checkpoint saved at {ema_checkpoint_path}")


def train_one_step(
    args,
    device,
    transformer,
    pipe,
    optimizer,
    lr_scheduler,
    batch,
    ema_handler,
):
    """
    Execute one training step using flow matching loss with gradient accumulation.

    Args:
        args: Training arguments
        device: Device to run on
        transformer: FLUX transformer model (FSDP wrapped)
        pipe: FLUX pipeline (for VAE encoding and utility functions)
        optimizer: Optimizer
        lr_scheduler: Learning rate scheduler
        batch: Batch data from dataloader
        ema_handler: EMA handler (optional)

    Returns:
        Tuple of (loss, grad_norm)
    """
    optimizer.zero_grad()
    transformer.train()

    # Unpack batch data
    (
        encoder_hidden_states,
        pooled_prompt_embeds,
        text_ids,
        caption,
        teacher_images,  # [batch_size, 3, H, W]
    ) = batch
    total_batch_size = teacher_images.shape[0]
    gradient_accumulation_steps = args.gradient_accumulation_steps

    # Calculate mini-batch size for each accumulation step
    mini_batch_size = total_batch_size // gradient_accumulation_steps
    accumulated_loss = 0.0

    teacher_images = teacher_images.to(device)

    # Encode teacher images to latents using pipeline's VAE (process entire batch at once)
    with torch.no_grad():
        # Encode images to latents using VAE
        target_latents = pipe.vae.encode(teacher_images.to(pipe.vae.dtype)).latent_dist.sample()
        target_latents = (target_latents - pipe.vae.config.shift_factor) * pipe.vae.config.scaling_factor

    # Get latent dimensions
    latent_h = target_latents.shape[2]
    latent_w = target_latents.shape[3]

    # Pack latents using pipeline's helper (process entire batch at once)
    packed_target_latents = pipe._pack_latents(
        target_latents,
        total_batch_size,
        num_channels_latents=target_latents.shape[1],
        height=latent_h,
        width=latent_w,
    )

    # Prepare image IDs using pipeline's helper (process entire batch at once)
    packed_latent_height = latent_h // 2
    packed_latent_width = latent_w // 2
    image_ids = pipe._prepare_latent_image_ids(
        total_batch_size,
        packed_latent_height,
        packed_latent_width,
        device,
        torch.bfloat16,
    )

    # Sample timestep for flow matching (for entire batch)
    # FLUX uses timesteps in [0, 1] range
    u = torch.rand(total_batch_size, device=device, dtype=torch.bfloat16)

    # Apply time shift if specified (for training stability)
    if args.shift > 1.0:
        u = (args.shift * u) / (1 + (args.shift - 1) * u)

    # Sample noise (for entire batch)
    noise = torch.randn_like(packed_target_latents)

    noisy_latents = (1.0 - u.view(-1, 1, 1)) * packed_target_latents + u.view(-1, 1, 1) * noise

    # Gradient accumulation loop - only forward/backward passes
    for accum_step in range(gradient_accumulation_steps):
        # Get mini-batch slice indices
        start_idx = accum_step * mini_batch_size
        end_idx = start_idx + mini_batch_size

        # Slice the prepared data
        mini_noisy_latents = noisy_latents[start_idx:end_idx]
        mini_encoder_hidden_states = encoder_hidden_states[start_idx:end_idx]
        mini_u = u[start_idx:end_idx]
        mini_text_ids = text_ids[start_idx:end_idx]
        mini_pooled_prompt_embeds = pooled_prompt_embeds[start_idx:end_idx]
        mini_noise = noise[start_idx:end_idx]
        mini_packed_target_latents = packed_target_latents[start_idx:end_idx]

        # Forward pass through transformer
        with torch.autocast("cuda", torch.bfloat16):
            model_pred = transformer(
                hidden_states=mini_noisy_latents,
                encoder_hidden_states=mini_encoder_hidden_states,
                timestep=mini_u,  # FLUX expects timestep in [0, 1]
                guidance=torch.full((mini_batch_size,), args.guidance_scale, device=mini_noisy_latents.device, dtype=torch.bfloat16),
                txt_ids=mini_text_ids[0],
                pooled_projections=mini_pooled_prompt_embeds,
                img_ids=image_ids,
                joint_attention_kwargs=None,
                return_dict=False,
            )[0]

        # Flow matching loss: predict velocity v = x_0 (noise) - x_1 (target)
        # The flow goes from x_1 (data) at t=1 to x_0 (noise) at t=0
        target_velocity = mini_noise - mini_packed_target_latents
        loss = F.mse_loss(model_pred.float(), target_velocity.float(), reduction="mean")

        # Scale loss for gradient accumulation
        scaled_loss = loss / gradient_accumulation_steps
        accumulated_loss += loss.item()

        # Backward pass
        scaled_loss.backward()

    # Gradient clipping
    grad_norm = transformer.clip_grad_norm_(args.max_grad_norm)

    # Optimizer step
    optimizer.step()
    lr_scheduler.step()

    # Calculate average loss across accumulation steps
    avg_accumulated_loss = accumulated_loss / gradient_accumulation_steps

    # Average loss across all processes
    avg_loss_tensor = torch.tensor(avg_accumulated_loss, device=device)
    dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.AVG)

    return avg_loss_tensor.item(), grad_norm.item()


def main(args):
    torch.backends.cuda.matmul.allow_tf32 = True

    local_rank = int(os.environ["LOCAL_RANK"])
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    dist.init_process_group("nccl")
    torch.cuda.set_device(local_rank)
    device = torch.cuda.current_device()
    initialize_sequence_parallel_state(args.sp_size)

    # Set seed for reproducibility
    if args.seed is not None:
        set_seed(args.seed + rank)

    # Create output directory
    if rank <= 0 and args.output_dir is not None:
        os.makedirs(args.output_dir, exist_ok=True)

    main_print(f"--> Loading model from {args.pretrained_model_name_or_path}")

    # Load FLUX pipeline (for VAE and utility functions)
    # We'll use the pipeline's VAE and helper methods, but replace the transformer
    pipe = FluxPipeline.from_pretrained(
        args.pretrained_model_name_or_path,
        torch_dtype=torch.bfloat16
    ).to(device)

    # Set VAE and text encoders to eval mode (only train transformer)
    pipe.vae.eval()
    pipe.vae.requires_grad_(False)
    pipe.text_encoder.eval()
    pipe.text_encoder.requires_grad_(False)
    pipe.text_encoder_2.eval()
    pipe.text_encoder_2.requires_grad_(False)

    # Encode negative prompt embeddings for CFG
    main_print(f"--> Encoding negative prompt: '{args.negative_prompt}'")
    with torch.no_grad():
        (
            negative_prompt_embeds,
            negative_pooled_prompt_embeds,
            _,
        ) = pipe.encode_prompt(
            prompt=args.negative_prompt,
            prompt_2=args.negative_prompt,
            device=device,
            num_images_per_prompt=1,
            prompt_embeds=None,
            pooled_prompt_embeds=None,
            max_sequence_length=args.max_sequence_length,
        )
        # Move to CPU for dataset storage
        negative_prompt_embeds = negative_prompt_embeds.cpu()
        negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.cpu()

    # Load transformer separately for FSDP wrapping (in float32 for training)
    transformer = FluxTransformer2DModel.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="transformer",
        torch_dtype=torch.float32
    )

    # Wrap transformer with FSDP
    fsdp_kwargs, no_split_modules = get_dit_fsdp_kwargs(
        transformer,
        args.fsdp_sharding_startegy,
        False,
        args.use_cpu_offload,
        args.master_weight_type,
    )

    transformer = FSDP(transformer, **fsdp_kwargs)

    # Replace pipeline's transformer with FSDP-wrapped version
    pipe.transformer = transformer

    # Initialize EMA if requested
    ema_handler = None
    if args.use_ema:
        ema_handler = FSDP_EMA(transformer, args.ema_decay, rank)

    # Apply gradient checkpointing
    if args.gradient_checkpointing:
        apply_fsdp_checkpointing(
            transformer, no_split_modules, args.selective_checkpointing
        )

    main_print(f"--> Model loaded")

    # Set model as trainable
    transformer.train()

    # Setup optimizer
    params_to_optimize = transformer.parameters()
    params_to_optimize = list(filter(lambda p: p.requires_grad, params_to_optimize))

    optimizer = torch.optim.AdamW(
        params_to_optimize,
        lr=args.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay,
        eps=1e-8,
    )

    main_print(f"Optimizer: {optimizer}")

    # Setup learning rate scheduler
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps,
        num_training_steps=args.max_train_steps,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
    )

    # Setup dataset
    train_dataset = FluxSFTLatentDataset(
        args.data_json_path,
        args.num_latent_t,
        args.cfg,
        args.teacher_image_root,
        target_image_size=args.image_size,
        max_images_per_prompt=args.max_images_per_prompt,
        negative_prompt_embeds=negative_prompt_embeds,
        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
    )

    sampler = DistributedSampler(
        train_dataset, rank=rank, num_replicas=world_size, shuffle=True, seed=args.sampler_seed
    )

    train_dataloader = DataLoader(
        train_dataset,
        sampler=sampler,
        collate_fn=flux_sft_latent_collate_function,
        pin_memory=True,
        batch_size=args.train_batch_size,
        num_workers=args.dataloader_num_workers,
        drop_last=True,
    )

    # Initialize wandb
    if rank <= 0:
        wandb.init(project=args.wandb_project_name, name=args.wandb_experiment_name, config=args)

    # Print training info
    total_batch_size = world_size * args.train_batch_size / args.sp_size * args.train_sp_batch_size
    main_print("***** Running SFT training *****")
    main_print(f"  Num examples = {len(train_dataset)}")
    main_print(f"  Dataloader size = {len(train_dataloader)}")
    main_print(f"  Instantaneous batch size per device = {args.train_batch_size}")
    main_print(f"  Total train batch size (w. data & sequence parallel) = {total_batch_size}")
    main_print(f"  Total optimization steps per epoch = {len(train_dataloader)}")
    main_print(f"  Max training steps = {args.max_train_steps}")
    main_print(
        f"  Total training parameters per FSDP shard = {sum(p.numel() for p in transformer.parameters() if p.requires_grad) / 1e9} B"
    )
    main_print(f"  Master weight dtype: {next(transformer.parameters()).dtype}")

    progress_bar = tqdm(
        range(0, args.max_train_steps),
        initial=0,
        desc="Steps",
        disable=local_rank > 0,
    )

    step_times = deque(maxlen=100)
    total_step = 0

    # Training loop - epoch-based with max_train_steps early stopping
    for epoch in range(args.num_epochs):
        if isinstance(sampler, DistributedSampler):
            sampler.set_epoch(epoch)
        # Iterate through all batches in the epoch
        for batch in train_dataloader:
            total_step += 1
            start_time = time.time()

            # Save checkpoint
            if total_step % args.checkpointing_steps == 0:
                save_checkpoint(transformer, rank, args.output_dir, total_step, epoch)
                if args.use_ema:
                    save_ema_checkpoint(ema_handler, rank, args.output_dir, total_step, epoch, dict(transformer.config))
                dist.barrier()

            # Training step
            loss, grad_norm = train_one_step(
                args,
                device,
                transformer,
                pipe,
                optimizer,
                lr_scheduler,
                batch,
                ema_handler,
            )

            # Update EMA
            if args.use_ema and ema_handler:
                ema_handler.update(transformer)

            # Log timing
            step_time = time.time() - start_time
            step_times.append(step_time)
            avg_step_time = sum(step_times) / len(step_times)

            progress_bar.set_postfix(
                {
                    "epoch": epoch,
                    "loss": f"{loss:.4f}",
                    "step_time": f"{step_time:.2f}s",
                    "grad_norm": f"{grad_norm:.4f}",
                }
            )
            progress_bar.update(1)

            if rank <= 0:
                wandb.log(
                    {
                        "train_loss": loss,
                        "learning_rate": lr_scheduler.get_last_lr()[0],
                        "step_time": step_time,
                        "avg_step_time": avg_step_time,
                        "grad_norm": grad_norm,
                        "epoch": epoch,
                    },
                    step=total_step,
                )

            # Check if we've reached max training steps
            if total_step >= args.max_train_steps:
                main_print(f"Reached max_train_steps={args.max_train_steps}, stopping training.")
                break

        # Break outer loop if max steps reached
        if total_step >= args.max_train_steps:
            break

    if get_sequence_parallel_state():
        destroy_sequence_parallel_group()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    # Dataset & dataloader
    parser.add_argument("--data_json_path", type=str, required=True)
    parser.add_argument("--teacher_image_root", type=str, required=True, help="Root directory containing teacher images")
    parser.add_argument("--dataloader_num_workers", type=int, default=10)
    parser.add_argument("--train_batch_size", type=int, default=16)
    parser.add_argument("--num_latent_t", type=int, default=1)
    parser.add_argument("--image_size", type=int, default=1024, help="Target image size")
    parser.add_argument("--max_images_per_prompt", type=int, default=8, help="Max teacher images per prompt")

    # Model paths
    parser.add_argument("--pretrained_model_name_or_path", type=str, required=True)
    parser.add_argument("--cache_dir", type=str, default="./cache_dir")

    # Training config
    parser.add_argument("--cfg", type=float, default=0.0, help="CFG rate for prompt dropout during training")
    parser.add_argument("--negative_prompt", type=str, default="", help="Negative prompt for CFG training")
    parser.add_argument("--max_sequence_length", type=int, default=512, help="Maximum sequence length for text encoding")
    parser.add_argument("--ema_decay", type=float, default=0.995)
    parser.add_argument("--use_ema", action="store_true", help="Enable EMA")
    parser.add_argument("--shift", type=float, default=1.0, help="Timestep shift for training")
    parser.add_argument("--guidance_scale", type=float, default=1.0, help="Parameter cfg")

    # Output & logging
    parser.add_argument("--seed", type=int, default=None)
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory for checkpoints")
    parser.add_argument("--checkpointing_steps", type=int, default=500)
    parser.add_argument("--logging_dir", type=str, default="logs")

    # Optimizer & scheduler
    parser.add_argument("--max_train_steps", type=int, default=10000)
    parser.add_argument("--num_epochs", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    parser.add_argument("--lr_warmup_steps", type=int, default=100)
    parser.add_argument("--max_grad_norm", default=1.0, type=float)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="Number of gradient accumulation steps")
    parser.add_argument("--gradient_checkpointing", action="store_true")
    parser.add_argument("--selective_checkpointing", type=float, default=1.0)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--master_weight_type", type=str, default="fp32")

    # Distributed training
    parser.add_argument("--use_cpu_offload", action="store_true")
    parser.add_argument("--sp_size", type=int, default=1)
    parser.add_argument("--train_sp_batch_size", type=int, default=1)
    parser.add_argument("--fsdp_sharding_startegy", default="full")
    parser.add_argument("--sampler_seed", type=int, default=42)

    # LR scheduler
    parser.add_argument("--lr_scheduler", type=str, default="constant")
    parser.add_argument("--lr_num_cycles", type=int, default=1)
    parser.add_argument("--lr_power", type=float, default=1.0)

    # Wandb
    parser.add_argument("--wandb_project_name", type=str, default='dancegrpo')
    parser.add_argument("--wandb_experiment_name", type=str, default='flux_sft')

    args = parser.parse_args()
    main(args)
